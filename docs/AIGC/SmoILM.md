---
sidebar_position: 10
---

# 小模型的训练模板--SmoILM2
近期，huggingface的团队推出了在多个评测数据集分数上超过Qwen2-1.5B的”小“模型--1.7B大小的SmoILM2，并且公开了训练数据集和训练方法，赶紧来看下，模型还是采用了和Llama2一样的架构，可以说是非常标准的训练模板，具有不错的学习借鉴意义。

## 研究背景
最近大参数量的大模型如Deepseek等取得了很大突破，但训练和推理成本相对来说还是高昂。于是huggingface团队近期研究致力于开发性能良好的小型语言模型，以降低计算成本并在更多设备上运行。这对于我们这些个人用户也是一个好消息。  
数据是影响语言模型性能的关键因素，对于小型模型尤为重要。这篇论文证明了除网络文本外，纳入专业数据可提升模型在推理和世界知识相关任务上的表现。  
## 模型训练
### 预训练数据集
文章在提高训练数据集的质量上花费了很多笔墨，也给我们提供了一些不错的思路。
- 首先，对现有的数据集（FineWeb-Edu, DCLM, 数据数据OpenWebMath, InfiMM-WebMath)进行了消融实验来得出最佳的混合比例。具体来说就是在相同的条件下在每个数据集上训练模型，在多种基准测试评估，测试不同混合比例，最终得出 60% FineWeb-Edu 和 40% DCLM 混合效果较好的结论。
- 对于数学数据OpenWebMath, InfiMM-WebMath，也是先做了退火消融实验，在GSM8K和MATH等任务上评估模型性能。发现 InfiMM-WebMath 在 GSM8K 上峰值准确率达 14% 高于OWM的10%，而OWM在MATH上略胜一筹，但两者性能仍落后于先进的小型模型。针对上述问题开发了FineMath数据集。先从Common Crawl中提取相关文本。然后是筛选阶段，使用基于Llama-3.1-70B-Instruct注释训练的分类器，通过5分制提示筛选出具有推理过程和初高中至本科阶段内容的页面。再使用MinHash LSH去重，并通过fastText语言分类保留英文内容，去除低质量数据，最终得到多个 FineMath 变体，如 FineMath4 + 和 FineMath3+。下图是这些数据集在不同基准测试中的表现。

### 预训练过程：
采用多阶段训练方法，在 11 万亿 token 上训练 SmolLM2。根据性能驱动调整数据集混合，在退火阶段对高质量数学和代码数据进行上采样，并在训练中期引入中等规模数据集。
训练设置：基于 Llama2 架构构建 17 亿参数的基础模型，使用特定的优化器和学习率调度进行训练，训练过程分为稳定阶段和衰减阶段，不同阶段采用不同的数据集混合策略。
模型评估
基础模型评估：将 SmolLM2 与类似规模的模型进行比较，在多个基准测试中表现出色，如在 HellaSwag、ARC 等任务上优于 Qwen2.5 - 1.5B，在数学和编码基准测试中也具有竞争力。
指令调整模型评估：使用新的指令跟随数据集 SmolTalk 对模型进行监督微调，再通过直接偏好优化进行对齐。最终的指令调整模型在指令跟随、推理和数学能力方面表现强劲。
其他规模模型：还训练了 SmolLM2 - 360M 和 SmolLM2 - 135M 两个较小的模型，它们在各自规模类别中也处于领先地位，采用了不同的数据处理和训练策略。
研究结论：SmolLM2 通过精心策划数据集和多阶段训练，在小型语言模型中取得了领先成果。开发的新数据集改善了模型在推理、数学和指令跟随任务中的能力，并发布了模型、数据集和代码，为未来研究提供支持。