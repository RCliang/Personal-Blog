---
sidebar_position: 12
---


# Qwen2.5-VL技术报告解读
“Qwen2.5-VL Technical Report” 由阿里云的 Qwen 团队撰写。该报告介绍了 Qwen2.5-VL 这一视觉语言模型系列的技术细节、优势及应用潜力，其在多方面取得显著进展，为推动视觉语言模型发展提供了新方向。
研究背景：大型视觉语言模型（LVLMs）整合视觉感知与自然语言处理，推动人工智能发展，但当前模型存在计算复杂、上下文理解有限、细粒度视觉感知差和处理不同长度序列性能不稳定等问题。Qwen2.5-VL 旨在探索细粒度感知能力，提升模型性能。
模型方法
模型架构：由大语言模型、视觉编码器和基于 MLP 的视觉语言合并器组成。视觉编码器采用重新设计的 ViT 架构，引入窗口注意力和 2D-RoPE，提升计算效率并有效捕捉空间关系；将 MRoPE 扩展到 3D 处理视频数据；通过 MLP 压缩图像特征序列，降低计算成本。
预训练：预训练数据从 1.2 万亿令牌扩展到约 4 万亿令牌，涵盖多种模态数据。对交错图像文本数据清洗提效，使用绝对位置坐标训练提升定位能力，合成多种文档数据增强文档解析能力，收集多源 OCR 数据提高 OCR 性能，动态采样处理视频数据，增强感知和决策数据提升智能体能力。预训练分三个阶段，逐步增强模型能力。
训练优化：采用 DataComp 和内部数据集训练视觉编码器，利用预训练的 Qwen2.5 LLM 初始化语言模型组件。预训练分三个阶段，使用不同数据和策略。训练中通过动态打包数据样本优化效率。
微调与对齐：采用监督微调（SFT）和直接偏好优化（DPO）的双阶段优化范式。SFT 使用精心策划的数据集，通过特定格式转换和数据过滤，使模型适应下游任务；DPO 利用偏好数据使模型符合人类偏好。
实验结果
综合能力评估：在大学水平问题、数学任务、通用视觉问答等多个任务上，Qwen2.5-VL-72B 与当前最先进模型相比表现出色，在多模态任务和纯文本任务上均展现强大实力。
专项能力评估：在通用视觉问答、文档理解与 OCR、空间理解、视频理解与定位和智能体能力等专项任务评估中，Qwen2.5-VL 系列模型在不同规模下均表现优异，部分任务超越同类模型。
研究结论：Qwen2.5-VL 在多模态理解和交互方面取得显著进展，不同规模的模型适用于多种场景，为视觉语言模型设定了新基准，其创新为更智能交互系统发展奠定基础s