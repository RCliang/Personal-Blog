---
sidebar_position: 11
---

# MLGym - 机器学习实验室？开源的机器学习助手？
[论文链接](https://arxiv.org/pdf/2502.14499)
“MLGym: A New Framework and Benchmark for Advancing AI Research Agents” 由 Deepak Nathani 等人撰写。文章介绍了 Meta MLGym 和 MLGym-Bench，这是用于评估和开发执行 AI 研究任务的大语言模型（LLM）智能体的新框架和基准，为推进 LLM 智能体的 AI 研究能力提供了重要支持。
## 背景与挑战：
AI 研究旨在加速科学发现，但目前缺乏评估 AI 智能体在开放式 AI 研究任务中能力的综合框架和基准。现有相关基准存在未涵盖开放式研究任务、研究领域狭窄、无法支持不同训练算法研究以及难以灵活评估研究成果等问题。
## MLGym 框架
### 设计理念：
受强化学习启发，构建了一个可在本地 Docker 容器中执行 Shell 命令的 Gym 环境，提供统一框架来评估和开发用于 AI 研究任务的智能体和模型。
### 核心组件：
包括智能体、环境、数据集和任务。智能体类封装基础 LLM，便于集成不同模型；环境基于 Gymnasium 设计，负责初始化 Shell 环境和管理交互；数据集支持本地和 Hugging Face 数据集，且与任务定义解耦；任务通过配置文件定义，可包含多个数据集、评估脚本等。
### 工具与 ACI：
扩展 SWE - Agent 的 ACI，增加文献搜索和内存模块等功能。提供验证、提交、文献搜索等工具，帮助智能体完成任务，其中内存模块可提升智能体在长期任务中的表现。
## MLGym-Bench 基准测试
- 任务设计：  
包含 13 个开放式研究任务，涵盖数据科学、博弈论、计算机视觉、自然语言处理和强化学习等领域，每个任务都有标准化评估脚本和基线实现。
- 具体任务：
如数据科学中的房价预测，博弈论中的囚徒困境、性别大战、Blotto 上校博弈，计算机视觉中的图像分类、图像字幕生成，自然语言处理中的自然语言推理、语言建模，强化学习中的 MetaMaze 导航、Mountain Car Continuous、Breakout MinAtar 等任务。
### 实验设置与评估
- 实验设置：
使用基于 SWE - Agent 并适配 MLGYM 环境的模型，选取 5 个前沿 LLM 进行实验，对 MLGYM 环境进行窗口配置、上下文管理等参数设置。
- 评估指标：
采用性能轮廓曲线和 AUP 分数来比较不同模型在多个任务上的相对性能，同时报告原始性能分数，并从最佳提交和最佳尝试两个维度进行评估。
## 实验结果
- 模型性能：
OpenAI O1 - preview 在综合性能上表现最佳，Gemini 1.5 Pro 和 Claude - 3.5 - Sonnet 紧随其后。Llama - 3.1 - 405b - instruct 和 GPT - 4o 在部分任务中表现不佳，如 Llama - 3.1 - 405b - instruct 在语言建模任务中，GPT - 4o 在 Breakout 任务中无法产生有效解决方案。
- 计算成本：
OpenAI O1 - preview 性能最佳但计算成本最高，Gemini 1.5 Pro 性价比最高，在性能和成本之间达到较好平衡。
- 智能体行为分析：
评估错误是模型最常遇到的终止错误。模型可靠性存在差异，GPT - 4o 失败率最高，Gemini 1.5 Pro 和 OpenAI O1 - preview 完成率最佳。智能体行动主要集中在编辑、查看文件，搜索命令使用较少。
## 研究结论：
MLGym 框架和 MLGym - Bench 任务表明现代 LLM 智能体可处理多种定量实验，但在跨领域扩展性、科学新颖性探索等方面仍存在不足。未来需改进评估方法，促进多领域合作，充分发挥 LLMs 在科学发现中的潜力。